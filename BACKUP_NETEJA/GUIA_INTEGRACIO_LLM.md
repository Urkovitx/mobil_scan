# üß† Guia d'Integraci√≥ LLM (Ollama + Phi-3)

## üìã Resum

Integraci√≥ d'un LLM local (Edge AI) al projecte Mobile Scanner per proporcionar respostes en llenguatge natural sobre productes detectats.

---

## üéØ Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend  ‚îÇ ‚Üê Usuari veu respostes LLM
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     API     ‚îÇ ‚Üê Pot consultar LLM
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Worker    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Ollama LLM ‚îÇ ‚Üê Phi-3 Model
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL ‚îÇ ‚Üê Taula products
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flux de Treball:

1. **Detecci√≥**: YOLOv8 detecta codi de barres
2. **Decodificaci√≥**: zxing-cpp llegeix el codi
3. **Consulta DB**: Busca producte a PostgreSQL
4. **Query LLM**: Envia info producte a Ollama
5. **Resposta**: LLM genera text en catal√†
6. **Guardar**: Resultat es guarda amb la detecci√≥

---

## üì¶ Components Creats

### 1. **docker-compose.llm.yml** ‚≠ê

Docker Compose actualitzat amb:
- Servei `llm` (Ollama)
- Servei `llm_init` (descarrega model Phi-3)
- Volum persistent `ollama_data`
- Variables d'entorn LLM_URL

### 2. **shared/init_db.sql**

Script SQL que crea:
- Taula `products` amb camps:
  - barcode (EAN-13)
  - name, description, category
  - price, stock, manufacturer
- 3 productes de prova:
  - Coca-Cola (5901234123457)
  - Danone Activia (8410076472106)
  - Oli d'Oliva (8480000123459)
- √çndexs i triggers

### 3. **shared/llm_client.py**

Client Python per Ollama amb:
- Classe `LLMClient`
- Funci√≥ `consultar_llm(product_info, user_question)`
- Prompts RAG-style
- Fallback si LLM no disponible
- Configuraci√≥ temperatura, tokens, etc.

### 4. **worker/processor_llm.py**

Worker actualitzat que:
- Detecta codis amb YOLO + zxing-cpp
- Consulta DB per info producte
- Crida LLM per resposta natural
- Guarda resposta LLM en fitxers .txt
- Mant√© compatibilitat amb worker original

---

## üöÄ Posada en Marxa

### Pas 1: Arreglar Docker Desktop

**IMPORTANT**: Primer assegura't que Docker funciona:

```cmd
ARREGLAR_DOCKER_PRIMER.bat
```

Si Docker est√† mort (error 500):
1. Tanca Docker Desktop (icona ‚Üí Quit)
2. Obre Task Manager (Ctrl+Shift+Esc)
3. Mata processos Docker si existeixen
4. Reobre Docker Desktop
5. Espera "Docker Desktop is running"

### Pas 2: Iniciar Serveis amb LLM

```cmd
docker-compose -f docker-compose.llm.yml up -d
```

**Temps estimat**: 
- Primera vegada: 15-20 minuts (descarrega model Phi-3 ~2.3GB)
- Seg√ºents vegades: 2-3 minuts (model ja descarregat)

### Pas 3: Verificar Estat

```cmd
docker-compose -f docker-compose.llm.yml ps
```

Hauries de veure:
- ‚úÖ mobil_scan_redis (Up)
- ‚úÖ mobil_scan_db (Up)
- ‚úÖ mobil_scan_llm (Up)
- ‚úÖ mobil_scan_api (Up)
- ‚úÖ mobil_scan_worker (Up)
- ‚úÖ mobil_scan_frontend (Up)
- ‚èπÔ∏è mobil_scan_llm_init (Exited 0) ‚Üê Normal, nom√©s descarrega model

### Pas 4: Verificar LLM

```cmd
curl http://localhost:11434/api/tags
```

Hauria de mostrar el model `phi3` instal¬∑lat.

### Pas 5: Verificar Base de Dades

```cmd
docker-compose -f docker-compose.llm.yml exec db psql -U mobilscan -d mobilscan_db -c "SELECT * FROM products;"
```

Hauria de mostrar els 3 productes de prova.

### Pas 6: Accedir a l'Aplicaci√≥

Obre: http://localhost:8501

---

## üß™ Provar la Integraci√≥

### Test 1: LLM Directe

```cmd
docker-compose -f docker-compose.llm.yml exec worker python -c "
from llm_client import LLMClient
client = LLMClient()
print('LLM Available:', client.is_available())

product = {
    'name': 'Coca-Cola 330ml',
    'description': 'Beguda refrescant',
    'price': 1.50,
    'stock': 150
}

response = client.consultar_llm(product)
print('Response:', response)
"
```

### Test 2: Consulta Producte

```cmd
docker-compose -f docker-compose.llm.yml exec worker python -c "
from database import SessionLocal
from sqlalchemy import text

db = SessionLocal()
result = db.execute(text('SELECT * FROM products WHERE barcode = :bc'), {'bc': '5901234123457'}).fetchone()
print('Product:', result)
db.close()
"
```

### Test 3: Pipeline Complet

1. Puja un v√≠deo amb un codi de barres EAN-13
2. El worker detectar√† el codi
3. Consultar√† la DB
4. Cridar√† el LLM
5. Guardar√† la resposta a `/app/results/{job_id}/llm_response_*.txt`

---

## üìä Configuraci√≥ del Model

### Model Utilitzat: **Phi-3** (Microsoft)

**Per qu√® Phi-3?**
- ‚úÖ Optimitzat per CPU/Edge
- ‚úÖ Nom√©s 2.3GB (vs 4-7GB d'altres models)
- ‚úÖ R√†pid en infer√®ncia
- ‚úÖ Bon rendiment en catal√†
- ‚úÖ Baix consum de mem√≤ria

### Alternatives:

Si vols canviar de model, edita `docker-compose.llm.yml`:

```yaml
llm_init:
  command: >
    sh -c "
    curl -X POST http://llm:11434/api/pull -d '{\"name\":\"llama3\"}';
    "
```

**Models recomanats**:
- `phi3` - 2.3GB, r√†pid, CPU-friendly ‚≠ê
- `llama3` - 4.7GB, m√©s potent, necessita m√©s RAM
- `mistral` - 4.1GB, bon balan√ß
- `gemma` - 2.5GB, similar a Phi-3

### Configuraci√≥ GPU (Opcional)

Si tens NVIDIA GPU, descomenta al `docker-compose.llm.yml`:

```yaml
llm:
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

---

## üîß Personalitzaci√≥

### Canviar el Prompt

Edita `shared/llm_client.py`, funci√≥ `_build_prompt()`:

```python
prompt = f"""Actua com un expert en productes.

PRODUCTE: {name}
PREU: {price}‚Ç¨
STOCK: {stock}

Proporciona una recomanaci√≥ professional.
"""
```

### Ajustar Par√†metres LLM

A `shared/llm_client.py`, funci√≥ `consultar_llm()`:

```python
"options": {
    "temperature": 0.7,  # 0.0 = determinista, 1.0 = creatiu
    "top_p": 0.9,        # Diversitat de resposta
    "max_tokens": 300    # Longitud m√†xima
}
```

### Afegir M√©s Productes

```sql
INSERT INTO products (barcode, name, description, category, price, stock, manufacturer) 
VALUES (
    '1234567890123',
    'Nom Producte',
    'Descripci√≥ detallada',
    'Categoria',
    9.99,
    100,
    'Fabricant'
);
```

---

## üìà Rendiment

### Recursos Necessaris:

| Component | CPU | RAM | Disc |
|-----------|-----|-----|------|
| Ollama + Phi-3 | 2 cores | 4GB | 3GB |
| Worker | 2 cores | 4GB | 1GB |
| PostgreSQL | 1 core | 512MB | 1GB |
| Redis | 1 core | 256MB | 100MB |
| **TOTAL** | **4-6 cores** | **8-10GB** | **5GB** |

### Temps de Resposta:

- **Detecci√≥ barcode**: ~100ms
- **Consulta DB**: ~10ms
- **Query LLM**: ~2-5 segons (CPU) / ~500ms (GPU)
- **Total per frame**: ~3-6 segons

### Optimitzacions:

1. **Cach√© de respostes**: Guarda respostes LLM per barcodes repetits
2. **Batch processing**: Processa m√∫ltiples deteccions juntes
3. **GPU**: Redueix temps LLM de 5s a 500ms
4. **Model m√©s petit**: Usa `phi3:mini` (1.5GB) si cal

---

## üêõ Troubleshooting

### Error: "LLM service not available"

```cmd
# Verificar que Ollama est√† actiu
docker-compose -f docker-compose.llm.yml ps llm

# Veure logs
docker-compose -f docker-compose.llm.yml logs llm

# Reiniciar servei
docker-compose -f docker-compose.llm.yml restart llm
```

### Error: "Model not found"

```cmd
# Descarregar model manualment
docker-compose -f docker-compose.llm.yml exec llm ollama pull phi3

# Verificar models instal¬∑lats
docker-compose -f docker-compose.llm.yml exec llm ollama list
```

### Error: "Out of memory"

Opcions:
1. Reduir `max_tokens` a 150
2. Usar model m√©s petit (`phi3:mini`)
3. Augmentar RAM de Docker Desktop (Settings ‚Üí Resources)
4. Tancar altres aplicacions

### Respostes LLM en angl√®s

Edita el prompt a `llm_client.py`:

```python
prompt += "\nIMPORTANT: Respon SEMPRE en catal√†, mai en angl√®s.\n"
```

---

## üìö Refer√®ncies

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Phi-3 Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

## ‚úÖ Checklist Final

- [ ] Docker Desktop funcionant
- [ ] `docker-compose.llm.yml` creat
- [ ] `shared/init_db.sql` creat
- [ ] `shared/llm_client.py` creat
- [ ] `worker/processor_llm.py` creat
- [ ] Serveis iniciats amb `docker-compose -f docker-compose.llm.yml up -d`
- [ ] Model Phi-3 descarregat (verificar amb `curl http://localhost:11434/api/tags`)
- [ ] Productes a la DB (verificar amb psql)
- [ ] LLM respon correctament (test amb llm_client.py)
- [ ] Aplicaci√≥ accessible a http://localhost:8501

---

**Ara tens un sistema complet amb IA Edge integrada!** üöÄüß†
